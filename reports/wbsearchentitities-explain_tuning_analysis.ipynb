{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "from gzip import GzipFile\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import bokeh.io\n",
    "import bokeh.plotting\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import relforge_wbsearchentities.tf_optimizer as opt\n",
    "from relforge.cli_utils import iterate_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "variables": {
     " variant ": "item_sv",
     "-variant-": "<p><strong>SyntaxError</strong>: invalid syntax (<ipython-input-305-565f89ee81e9>, line 1)</p>\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "variant = os.environ.get('WIKIDATA_VARIANT')\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "Wikidata entity completion - {variant}\n",
    "=================================\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bokeh.io.output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.environ.get(\n",
    "    'DATASET_DIR', \n",
    "    '/src/tf-ltr-data/WikidataCompletionClicks-202204')\n",
    "with GzipFile(BASE_DIR + '/model/model.{}.pkl.gz'.format(variant), 'rb') as f:\n",
    "    min_report = pickle.load(f)\n",
    "with GzipFile(BASE_DIR + '/sensitivity/sensitivity.{}.pkl.gz'.format(variant), 'rb') as f:\n",
    "    sens_report = pickle.load(f)\n",
    "context, language = variant.split('_')\n",
    "with GzipFile(BASE_DIR + '/source.pkl.gz', 'rb') as f:\n",
    "    all_source_data = pickle.load(f)\n",
    "    cond = (all_source_data['context'] == context) & (all_source_data['language'] == language)\n",
    "    source_data = all_source_data[cond]\n",
    "for report in min_report.evaluation_reports + [min_report.initial_report]:\n",
    "    report.scores = {\n",
    "        k: v if isinstance(v, opt.EvaluationScores) else opt.EvaluationScores(v) \n",
    "        for k, v in report.scores.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = source_data['dt'].min()\n",
    "# maybe not exact, but close enough\n",
    "test_train_mid = source_data['dt'].sort_values().iloc[min_report.num_observations['train']]\n",
    "test_end = source_data['dt'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.kde import gaussian_kde\n",
    "from collections import defaultdict\n",
    "\n",
    "def ridge(bucket, data, scale):\n",
    "    return list(zip([bucket]*len(data), scale*data))\n",
    "\n",
    "def plot_distribution(title, buckets, data, colors):\n",
    "    min_x = min(np.min(raw) for _, raw in data.values())\n",
    "    max_x = max(np.max(raw) for _, raw in data.values())\n",
    "    \n",
    "    x = np.linspace(min_x, max_x, 500)\n",
    "    # A bit evil .. but for the patch to draw the polygon we need\n",
    "    # the data to start and end with y=0. The first and last\n",
    "    # x values are repeated and these are applied manually later.\n",
    "    x = np.append(np.append(x, x[-1])[::-1], x[0])[::-1]\n",
    "    source = bokeh.models.ColumnDataSource(data=dict(x=x))\n",
    "    p = bokeh.plotting.figure(\n",
    "        y_range=sorted(buckets, reverse=True), title=title,\n",
    "        plot_height=75 * len(buckets), plot_width=700,\n",
    "        x_range=(min_x, max_x),\n",
    "        toolbar_location=None)\n",
    "    \n",
    "    pdfs = {bucket: gaussian_kde(raw) for bucket, (_, raw) in data.items()}\n",
    "    ys = {bucket: pdf(x) for bucket, pdf in pdfs.items()}\n",
    "    max_y = max(np.max(ys[bucket]) for bucket in data.keys())\n",
    "    scale = 0.8 / max_y\n",
    "    \n",
    "    bounds_data = defaultdict(list)\n",
    "    for bucket, (bounds, raw) in sorted(data.items(), key=lambda x: x[0], reverse=True):\n",
    "        # Apply polygon minimum edges\n",
    "        ys[bucket][0] = 0\n",
    "        ys[bucket][-1] = 0\n",
    "        y = ridge(bucket, ys[bucket], scale=scale)\n",
    "        source.add(y, bucket)\n",
    "        p.patch(\n",
    "            'x', bucket, color=colors[bucket], line_color=\"black\",\n",
    "            alpha=0.6, source=source)\n",
    "        if bounds:\n",
    "            bounds_data['buckets'].append(bucket)\n",
    "            bounds_data['upper'].append(bounds[-1])\n",
    "            bounds_data['lower'].append(bounds[0])\n",
    "    if bounds_data:\n",
    "        source_error = bokeh.models.ColumnDataSource(bounds_data)\n",
    "        p.add_layout(bokeh.models.Whisker(\n",
    "            dimension=\"width\", line_color=\"black\",\n",
    "            source=source_error, base=\"buckets\", upper=\"upper\", lower=\"lower\"))\n",
    "\n",
    "    p.y_range.range_padding = 0.4\n",
    "    bokeh.io.show(p)\n",
    "\n",
    "def ci(values, rounds=5000, alpha=0.05, n=None, agg=lambda x: x.mean(axis=1)):\n",
    "    if n is None:\n",
    "        n = len(values)\n",
    "    samples = np.random.choice(values, size=n * rounds, replace=True).reshape(rounds, -1)\n",
    "    scores = np.sort(agg(samples))\n",
    "    low = int(rounds * (alpha/2))\n",
    "    mid = int(rounds / 2)\n",
    "    high = int(rounds * (1 - alpha/2))\n",
    "    return (scores[low], scores[mid], scores[high]), scores\n",
    "\n",
    "def plot_ci(title, df, colors, extract, rounds=1000):\n",
    "    data = {}\n",
    "    buckets = df['bucket'].unique()\n",
    "    for bucket in sorted(buckets):\n",
    "        samples = extract(df[df['bucket'] == bucket])\n",
    "        data[bucket] = ci(samples, rounds=rounds)\n",
    "    plot_distribution(title, buckets, data, colors)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/45846841/5596181\n",
    "def human_format(num):\n",
    "    num = float('{:.3g}'.format(num))\n",
    "    magnitude = 0\n",
    "    while abs(num) >= 1000:\n",
    "        magnitude += 1\n",
    "        num /= 1000.0\n",
    "    return '{}{}'.format('{:f}'.format(num).rstrip('0').rstrip('.'), ['', 'K', 'M', 'B', 'T'][magnitude])\n",
    "\n",
    "def month_day(ts):\n",
    "    return ts.strftime('%b %d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "variables": {
     " human_format(min_report.num_observations['test']) ": "4.86K",
     " human_format(min_report.num_observations['train']) ": "4.86K",
     " month_day(test_end) ": "Mar 31",
     " month_day(test_train_mid) ": "Mar 17",
     " month_day(train_start) ": "Mar 01"
    }
   },
   "outputs": [],
   "source": [
    "display(Markdown(f\"\"\"\n",
    "Metric used\n",
    "===========\n",
    "\n",
    "The metric used here to compare different tuning of the autocomplete algorithm represents \n",
    "the probablistic number of characters typed by a typical user before selecting their \n",
    "desired result from the autocomplete drop down. This metric first looks at real user \n",
    "sessions to estimate how likely a user is to continue typing even when their result \n",
    "is presented in the autocomplete, conditioned on the position the result is displayed at.\n",
    "Individual user sessions, represented in the data as (prefix_typed, page_id_clicked), are\n",
    "then simulated with prefixes from length 1 to the full prefix typed. From this simulation\n",
    "we determine the expected number of characters typed for an individual search clickthrough.\n",
    "\n",
    "The dataset used contains {human_format(min_report.num_observations['train'])}\n",
    "clickthroughs from {month_day(train_start)} - {month_day(test_train_mid)} in\n",
    "the training set, and another {human_format(min_report.num_observations['test'])}\n",
    "clicks from {month_day(test_train_mid)} - {month_day(test_end)} in the test set.\n",
    "\n",
    "The graph below shows bootstrapped probability densities for each bucket. Tick marks are\n",
    "shown at the 95% confidence levels.\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "initial = min_report.initial_report['test'].scores\n",
    "best = min_report.best_report['test'].scores\n",
    "df = pd.DataFrame({\n",
    "    'bucket': (['initial'] * len(initial)) + (['best'] * len(best)),\n",
    "    'value': np.append(initial, best)\n",
    "})\n",
    "colors = {\n",
    "    'initial': 'blue',\n",
    "    'best': 'orange',\n",
    "}\n",
    "confidence = plot_ci('Mean characters typed probability density', df, colors, lambda x: x['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nearby_reports = [r for r in min_report.evaluation_reports if r.scores['test'].mean < confidence['best'][0][-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_run(a):\n",
    "    # start/stop index pairs for sequential True's\n",
    "    idx_pairs = np.where(np.diff(np.hstack(([False], a, [False]))))[0].reshape(-1, 2)\n",
    "    # lazily take first, might be too narrow in some cases\n",
    "    return idx_pairs[np.diff(idx_pairs,axis=1).argmax()]\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'initial': min_report.initial_report['test'].percentiles,\n",
    "    'best': min_report.best_report['test'].percentiles,\n",
    "}).reset_index()\n",
    "# smaller (less chars typed) is better. Area's of True show improvements greater\n",
    "# than half a char.\n",
    "major_improvement = longest_run((df['best'] + 0.5) < df['initial'])\n",
    "minor_improvement = longest_run(df['best'] < df['initial'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "variables": {
     " major_improvement[0] ": "42",
     " major_improvement[1] ": "60",
     " minor_improvement[1] ": "92"
    }
   },
   "outputs": [],
   "source": [
    "display(Markdown(f\"\"\"\n",
    "Mean characters typed by percentile\n",
    "==============================\n",
    "The following graphs show the before and after effects of tuning. This shows strong\n",
    "improvement, up to a full character, from percentiles \n",
    "{major_improvement[0]}-{major_improvement[1]} with minor improvements up to \n",
    "percentile {minor_improvement[1]}. Tail queries show some decline in performance,\n",
    "but only slightly.\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'initial': min_report.initial_report['test'].percentiles,\n",
    "    'best': min_report.best_report['test'].percentiles,\n",
    "}).reset_index()\n",
    "p = bokeh.plotting.figure()\n",
    "p.line('initial', 'index', source=df, legend='initial score')\n",
    "p.line('best', 'index', source=df, legend='best score', line_color='orange')\n",
    "p.xaxis.axis_label = 'expected characters typed'\n",
    "p.yaxis.axis_label = 'session percentile'\n",
    "bokeh.plotting.show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_delta = min_report.best_report.scores['test'].scores - min_report.initial_report['test'].scores\n",
    "percentiles = np.arange(0, 101)\n",
    "score_delta_percentile = np.percentile(score_delta, percentiles)\n",
    "per_session = {\n",
    "    \"major_plus\": (score_delta_percentile <= -1).sum(),\n",
    "    \"minor_plus\": ((score_delta_percentile < 0) & (score_delta_percentile > -1)).sum(),\n",
    "    \"no_change\": (score_delta_percentile == 0).sum(),\n",
    "    \"negative\": (score_delta_percentile > 0).sum(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "variables": {
     " per_session['major_plus'] ": "16",
     " per_session['minor_plus'] ": "44",
     " per_session['negative'] ": "17",
     " per_session['no_change'] ": "24"
    }
   },
   "outputs": [],
   "source": [
    "display(Markdown(f\"\"\"\n",
    "Per-session delta in expected characters typed\n",
    "======================================\n",
    "\n",
    "This shows on a per-session basis the change in number of characters typed between the \n",
    "baseline scoring and the scoring after parameter tuning. A session is defined as the user\n",
    "submitting a series of autocomplete queries for a single input. This suggests \n",
    "{per_session['major_plus']}% of sessions save between 1 and 6 characters typed. Another \n",
    "{per_session['minor_plus']}% save a fractional character, and {per_session['no_change']}%\n",
    "have no impact. Around {per_session['negative']}% of sessions are impacted negatively,\n",
    "future inspection of what makes these sessions different may be useful for investigating \n",
    "new scoring signals.\n",
    "\n",
    "The orange line shows the training run with the best score. The faint blue lines show other \n",
    "training runs that have a mean score less than the upper 95% CI of the best score.\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = bokeh.plotting.figure(title='expected characters typed delta')\n",
    "p.xaxis.axis_label = 'expected change in characters typed'\n",
    "p.yaxis.axis_label = 'session percentile'\n",
    "percentiles = np.arange(0, 101)\n",
    "best_report = min_report.best_report\n",
    "for report in nearby_reports:\n",
    "    if report == best_report:\n",
    "        continue\n",
    "    score_delta = report.scores['test'].scores - min_report.initial_report['test'].scores\n",
    "    score_delta_percentile = np.percentile(score_delta, percentiles)\n",
    "    p.line('delta', 'percentile',\n",
    "           source={'delta': score_delta_percentile, 'percentile': percentiles},\n",
    "           alpha=0.4)\n",
    "\n",
    "score_delta = best_report.scores['test'].scores - min_report.initial_report['test'].scores\n",
    "percentiles = np.arange(0, 101)\n",
    "score_delta_percentile = np.percentile(score_delta, percentiles)\n",
    "p.line('delta', 'percentile', \n",
    "       source={'delta': score_delta_percentile, 'percentile': percentiles},\n",
    "       color='orange', line_width=2, legend='best score')\n",
    "\n",
    "bokeh.plotting.show(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Tuned Values\n",
    "================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in sorted(min_report.summary['best_report']['variables'].items(), key=lambda x: x[0]):\n",
    "    if 'statement_keywords' in k:\n",
    "        # not worth tuning, these are simply deboosts that should be strong enough to do the job\n",
    "        continue\n",
    "    print('{score:10.2f} : {name}'.format(score=v, name=k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mean = min_report.best_report['test'].mean\n",
    "top_reports = [x for x in min_report.evaluation_reports if x['test'].mean <= confidence['best'][0][-1]]\n",
    "# [x['test'].mean for x in top_reports]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dict({'score': [x['test'].mean for x in min_report.evaluation_reports]}, **{\n",
    "    var_name: [x.variables[var_name] for x in min_report.evaluation_reports]\n",
    "    for var_name in min_report.best_report.variables.keys()\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"\"\"\n",
    "Sensitivity of chosen parameters\n",
    "===========================\n",
    "\n",
    "To get an idea of how much influence individual parameters have on the final score, and \n",
    "to estimate how sensitive those variables are to small changes, the graphs below plot the \n",
    "sensitivity of individual parameters. This is performed by holding all variables except \n",
    "one as a constant, and sweeping a set of values around the chosen point. The graphs then\n",
    "show how the final score changes based on changes to that variable. Dots on the graphs are \n",
    "additionally colored by their score. A graph of a single color suggests the variable in\n",
    "question has a relatively small influence on the final output.\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import bokeh.models\n",
    "from bokeh.palettes import Viridis256\n",
    "\n",
    "high = df['score'].mean() + df['score'].std()\n",
    "cmap = bokeh.models.LinearColorMapper(palette=Viridis256, low=np.min(df['score']), high=high)\n",
    "plots = []\n",
    "for var_name, reports in sorted(sens_report.variable_reports.items(), key=lambda x: x[0]):\n",
    "    tested_values = [r.variables[var_name] for r in reports]\n",
    "    scores = [r['test'].mean for r in reports]\n",
    "\n",
    "    p = bokeh.plotting.figure(title=var_name, height=250)\n",
    "    p.circle(x='value', y='score', size=8, alpha=0.5,\n",
    "             fill_color={'field': 'score', 'transform': cmap},\n",
    "             source={'value': tested_values, 'score': scores})\n",
    "    plots.append([p])\n",
    "grid = bokeh.layouts.gridplot(plots)\n",
    "bokeh.plotting.show(grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CirrusSearch Configuration\n",
    "======================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_php(value):\n",
    "    # Correct some of the time at least\n",
    "    return json.dumps(\n",
    "        value,\n",
    "        indent=4,\n",
    "        sort_keys=True,\n",
    "        separators=(',', ' => ')\n",
    "    ).replace('{', '[').replace('}', ']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from math import log10, floor\n",
    "\n",
    "def round_sig_fig(x, n=2):\n",
    "    return round(x, n-1-int(floor(log10(abs(x)))))\n",
    "\n",
    "assert round_sig_fig(0.1234, 1) == 0.1\n",
    "assert round_sig_fig(0.1234, 2) == 0.12\n",
    "assert round_sig_fig(123.4, 1) == 100\n",
    "assert round_sig_fig(123.4, 2) == 120\n",
    "\n",
    "profile = {}\n",
    "if language == \"en\":\n",
    "    profile['language-chain'] = ['en']\n",
    "else:\n",
    "    # Not always true\n",
    "    profile['language-chain'] = [language, 'en']\n",
    "function_chain = {}\n",
    "rescore = {\n",
    "    'i18n_msg': 'wikibase-rescore-profile-prefix',\n",
    "    'supported_namespaces': 'all',\n",
    "    'rescore': {\n",
    "        'window': 8192,\n",
    "        'window_size_override': 'EntitySearchRescoreWindowSize',\n",
    "        'score_mode': 'total',\n",
    "        'type': 'function_score',\n",
    "        'function_chain': 'wikibase_config_entity_weight',\n",
    "        'function_chain_overrides': function_chain,\n",
    "    }\n",
    "}\n",
    "for k, v in min_report.best_report.variables.items():\n",
    "    v = round_sig_fig(float(v), 3) # back to python types from numpy, make precision sane\n",
    "    parts = k.split('/')\n",
    "    if k == 'rescore/0/query_weight:0':\n",
    "        rescore['query_weight'] = v\n",
    "    elif k == 'rescore/0/rescore_query_weight:0':\n",
    "        rescore['rescore_query_weight'] = v\n",
    "    elif parts[0:2] == ['query', 'dismax']:\n",
    "        if parts[2].startswith('tie_breaker:'):\n",
    "            profile['tie-breaker'] = v\n",
    "            continue\n",
    "        idx = parts[2]\n",
    "        assert parts[3] == 'constant_score', k[3]\n",
    "        if parts[4].startswith('labels_all'):\n",
    "            profile['any'] = v\n",
    "            continue\n",
    "        lang, field = parts[4].split('.')[1:3]\n",
    "        norm_field = {\n",
    "            'near_match': 'exact',\n",
    "            'near_match_folded': 'folded',\n",
    "            'prefix': 'prefix'\n",
    "        }[field]\n",
    "        profile[f\"{lang}-{norm_field}\"] = v\n",
    "    elif parts[0] == 'query':\n",
    "        # This should have been excluded from training, it's for exact match\n",
    "        # on Q items and only needs \"enough\" weight.\n",
    "        pass\n",
    "    elif parts[0:2] == ['rescore', '0']:\n",
    "        assert parts[2] == 'function_score'\n",
    "        idx = parts[3]\n",
    "        if parts[4] == 'weight:0':\n",
    "            function_chain[f\"functions.{idx}.weight\"] = v\n",
    "        elif parts[4] == 'satu':\n",
    "            param = parts[6].split(':')[0]\n",
    "            function_chain[f\"functions.{idx}.params.{param}\"] = v\n",
    "        else:\n",
    "            assert False\n",
    "    else:\n",
    "        assert False, k\n",
    "\n",
    "date = BASE_DIR.split('-')[-1] # hax\n",
    "profile_var = f\"$wgWBCSPrefixSearchProfiles['wikibase_config_prefix_query-{date}-{language}'] = \"\n",
    "print(profile_var + as_php(profile))\n",
    "rescore_var = f\"$wgWBCSRescoreProfiles['wikibase_config_entity_weight-{date}-{language}'] = \"\n",
    "print(rescore_var + as_php(rescore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
